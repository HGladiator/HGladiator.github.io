---
layout: post
title: IDEA编写Scala过程中遇到的若干问题
categories: Spark
tags: 工具
excerpt: IDEA这个Scala官方推荐的IDE是很好用的，但是在使用过程中会遇到很多问题，特别是刚开始的上手的时候会遇到各种各样让人头大的问题，这里记录一下我遇到的问题。
---

* content
{:toc}


0. 依赖下载速度问题

```
    //sbt-launch.jar 包用WinRAR 打开后改写sbt.boot.properties文件，添加下面两行
  aliyun-nexus: http://maven.aliyun.com/nexus/content/groups/public/
  jcenter: http://jcenter.bintray.com/
```
	然后用 jar -cfM jarname.jar  . 打包然后替换
	在`project setting`-`Modules`-`your project name-build`-`SBT`-`Resolvers`栏里能看到添加的依赖网址就算成功了
```
  // 在built.sbt文件中添加
  resolvers+="OS China" at "http://maven.oschina.net/content/groups/public/"
```
1. Exception in thread "main" java.lang.NoClassDefFoundError: scala/Product$class
依赖不匹配  要spark2.2.0 对应的scala版本依赖包，而spark2.2.0才出没几天，spark2.2.0 压缩包里的还是2.1.0的库

2. Internal error: Scala instance doesn't exist or is invalid:
    version unknown, library jar: C:\Users\username\.ivy2\cache\jline\jline\jars\jline-2.14.3.jar, compiler jar: C:\Users\username\.ivy2\cache\org.scala-lang.modules\scala-parser-combinators_2.11\bundles\scala-parser-combinators_2.11-1.0.4.jar   
sbt 不匹配 有BUG 1.0.3修复


3. Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.$conforms()Lscala/Predef$$less$colon$less;
不清楚，应该是 项目列表中src 莫名消失的原因

4. main scala test mian 下面的  Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/sql/SparkSession$
而同样的代码 在test 下可以运行
    - 在项目全局库 添加jar包   问题原因可能是项目jar包配置问题
    - 把包直接导入到项目的库中
    - project structrue 中点修复

5. IDEA项目中  sbt打包测试ClassNotFoundException 应该是要放到lib目录下   build.sbt  resolvers += "Local Maven Repository" at "file://C://Users//username//.ivy2//cache"   不管用
而IDEA build 打包 可以运行

6. run configuration  - VM options : -Dspark.master=local

7. java.net.UnknownHostException:  123.123.1.1
zk的configuration 字符串中ip要连在一起 不能有空格

8. Spark2.x读取Hbase 到Dataframe  
    - 网上没有找到这个适配这个版本的处理方式
    - 把读取的数据记录下来 自己转换卡在hbase scanner 没有rowkey
    - 找第三方没找到在Spark2.X下运行的 第三方好像都是Catalog

9. Spark DataFrame 星座字符串转数值 添加到新列
    - 通过withColumn 添加新列 把旧的列进行map  发现错误
    found   : org.apache.spark.sql.Dataset[Int]
    required: org.apache.spark.sql.Column
    - 通过map 得到新建的一个df 然后合并 报错 因为没有 共同的key
    - 写sql 合并表格  也需要相应的key
    - 直接在csv表格中修改

10. 遇到问题不仅要学会查Google、stack overflow，有时候查阅相关的FAQ/spark项目中的demo、example，还有运行产生的日志


